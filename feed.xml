<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://yurekami.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://yurekami.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-14T13:30:12+00:00</updated><id>https://yurekami.github.io/feed.xml</id><title type="html">blank</title><subtitle>First principles thinking. Computational modeling. Mathematics. </subtitle><entry><title type="html">The Mathematical Metabolism: Why AI Solves Problems But Doesn’t Create Mathematics</title><link href="https://yurekami.github.io/blog/2026/mathematical-metabolism/" rel="alternate" type="text/html" title="The Mathematical Metabolism: Why AI Solves Problems But Doesn’t Create Mathematics"/><published>2026-02-14T00:00:00+00:00</published><updated>2026-02-14T00:00:00+00:00</updated><id>https://yurekami.github.io/blog/2026/mathematical-metabolism</id><content type="html" xml:base="https://yurekami.github.io/blog/2026/mathematical-metabolism/"><![CDATA[<p>Google’s Aletheia system solved four open Erdos conjectures. Headlines called it a breakthrough. And it is — in the same way that a calculator performing long division was a breakthrough. The 0.6% solve rate on 700 Erdos problems is both the headline and the sobriety check: impressive enough to matter, low enough to demand scrutiny.</p> <p>But the real question isn’t about solve rates.</p> <blockquote> <p>“What I cannot create, I do not understand.” — Richard Feynman</p> </blockquote> <p>Feynman’s dictum cuts both ways. Invert it: <strong>what creates nothing, understands nothing — no matter how many problems it solves.</strong> Aletheia produces proofs. It does not produce mathematics. The distinction is the entire point of this post.</p> <hr/> <h2 id="1-the-three-stage-dissection">1. The Three-Stage Dissection</h2> <p>I applied the First Proof methodology to Google’s announcement — a structured analysis in three stages: <strong>Formulation</strong> (what is actually being claimed?), <strong>Framework</strong> (how does it compare to prior work?), and <strong>Execution</strong> (what survives scrutiny?).</p> <p>The architecture itself is straightforward. It follows the generator-verifier pattern that has become standard in AI-for-math systems since AlphaProof:</p> <pre><code class="language-mermaid">graph TD
    P[Problem Input] --&gt; G[Generator&lt;br/&gt;Gemini Deep Think]
    G --&gt; C[Candidate Proof]
    C --&gt; V[Verifier&lt;br/&gt;Gemini + Heuristics]
    V --&gt; |Accept| O[Output Solution]
    V --&gt; |Minor Flaw| MF[Minor Fix Loop]
    MF --&gt; G
    V --&gt; |Critical Flaw| CF[Discard &amp; Retry]
    CF --&gt; G
    V --&gt; |Ambiguous| HE[Human Expert Review]

    style G fill:#0076df,color:#fff
    style V fill:#ffaa00,color:#000
    style O fill:#00ab37,color:#fff
    style CF fill:#ff3636,color:#fff
    style HE fill:#b509ac,color:#fff
</code></pre> <p>Five claims emerge from the announcement:</p> <ol> <li><strong>Autonomous Erdos Solving</strong> — the system independently solved four open conjectures.</li> <li><strong>Scaling Beyond Olympiad</strong> — this extends AI math reasoning past competition-level problems.</li> <li><strong>Agentic Scaffold Efficiency</strong> — the generator-verifier loop is an effective reasoning scaffold.</li> <li><strong>Cross-Domain Transfer</strong> — capabilities generalize across mathematical domains.</li> <li><strong>Responsible Taxonomy</strong> — solutions are categorized by level of human involvement.</li> </ol> <p>Each claim deserves independent evaluation. Not all survive.</p> <hr/> <h2 id="2-multi-perspective-analysis">2. Multi-Perspective Analysis</h2> <p>I ran parallel analyses through different lenses — a factual assessment, a senior engineer’s evaluation, a security-minded adversarial read, and a consistency check. The convergence was instructive: all analyses agreed on where the evidence was strong and where it evaporated.</p> <pre><code class="language-plotly">{
  "data": [{
    "type": "scatterpolar",
    "r": [30, 45, 60, 65, 55],
    "theta": ["Autonomous Erdos Solving", "Scaling Beyond Olympiad", "Agentic Scaffold Efficiency", "Cross-Domain Transfer", "Responsible Taxonomy"],
    "fill": "toself",
    "name": "Evidence Strength (%)",
    "line": {"color": "#0076df"}
  }, {
    "type": "scatterpolar",
    "r": [90, 70, 85, 75, 50],
    "theta": ["Autonomous Erdos Solving", "Scaling Beyond Olympiad", "Agentic Scaffold Efficiency", "Cross-Domain Transfer", "Responsible Taxonomy"],
    "fill": "toself",
    "name": "What's Missing (%)",
    "line": {"color": "#ff3636"}
  }],
  "layout": {
    "polar": {"radialaxis": {"visible": true, "range": [0, 100]}},
    "title": {"text": "Evidence Strength vs. Information Gaps Across 5 Claims"},
    "showlegend": true,
    "font": {"size": 12}
  }
}
</code></pre> <p>The pattern is stark. The strongest evidence exists for the <strong>scaffold design</strong> and the <strong>taxonomy</strong> — engineering claims, not mathematical ones. The weakest evidence exists for the headline claim: autonomous solving. The information gaps are largest precisely where the claims are boldest.</p> <p>This is not unusual. It is the standard pattern of AI capability announcements: strong engineering, weak epistemics.</p> <hr/> <h2 id="3-the-hypothesis-landscape">3. The Hypothesis Landscape</h2> <p>What is actually happening when Aletheia “solves” an Erdos conjecture? Five competing hypotheses, each with different implications:</p> <ul> <li><strong>H1: Genuine Reasoning</strong> — the model performs something functionally equivalent to mathematical reasoning, discovering novel proof strategies.</li> <li><strong>H2: Sophisticated Pattern Matching</strong> — the model recombines known techniques in ways that happen to work on these particular problems, without genuine understanding.</li> <li><strong>H3: Scaffold Innovation</strong> — the breakthrough is not in the model but in the agentic architecture: the feedback loops, the verification, the retry logic.</li> <li><strong>H4: Selection Bias</strong> — the solved problems were particularly amenable to existing techniques; the 99.4% failure rate is the real signal.</li> <li><strong>H5: Human Laundering</strong> — human expertise entered the pipeline through problem selection, hint provision, or verification criteria, and the “autonomous” label obscures this.</li> </ul> <p>I estimated probabilities from two independent analysis runs:</p> <pre><code class="language-vega_lite">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "width": 500,
  "height": 300,
  "title": "What's Actually Happening? Two Independent Probability Estimates",
  "data": {
    "values": [
      {"hypothesis": "H1: Genuine Reasoning", "run": "Opus", "probability": 15},
      {"hypothesis": "H1: Genuine Reasoning", "run": "Sonnet", "probability": 15},
      {"hypothesis": "H2: Pattern Matching", "run": "Opus", "probability": 35},
      {"hypothesis": "H2: Pattern Matching", "run": "Sonnet", "probability": 30},
      {"hypothesis": "H3: Scaffold Innovation", "run": "Opus", "probability": 30},
      {"hypothesis": "H3: Scaffold Innovation", "run": "Sonnet", "probability": 25},
      {"hypothesis": "H4: Selection Bias", "run": "Opus", "probability": 10},
      {"hypothesis": "H4: Selection Bias", "run": "Sonnet", "probability": 20},
      {"hypothesis": "H5: Human Laundering", "run": "Opus", "probability": 10},
      {"hypothesis": "H5: Human Laundering", "run": "Sonnet", "probability": 10}
    ]
  },
  "mark": {"type": "bar", "cornerRadiusTopLeft": 3, "cornerRadiusTopRight": 3},
  "encoding": {
    "x": {"field": "hypothesis", "type": "nominal", "axis": {"labelAngle": -25, "title": null}},
    "y": {"field": "probability", "type": "quantitative", "title": "Probability (%)"},
    "color": {"field": "run", "type": "nominal", "scale": {"range": ["#0076df", "#00ab37"]}, "title": "Analysis Run"},
    "xOffset": {"field": "run"}
  }
}
</code></pre> <p>The convergence is notable. Both runs assign the highest probability to <strong>pattern matching</strong> (H2) and <strong>scaffold innovation</strong> (H3), with genuine reasoning (H1) at only 15%. The modal explanation is a combination: clever engineering makes sophisticated pattern matching look like reasoning on a carefully selected subset of problems.</p> <p>This is not a dismissal. Pattern matching at this level of sophistication is genuinely useful. But it is not mathematics.</p> <hr/> <h2 id="4-the-verification-catastrophe">4. The Verification Catastrophe</h2> <p>The arxiv paper contains the actual numbers. They are worse than the announcement suggests.</p> <p>Of 700 Erdos problems attempted, the verifier flagged 212 as “correct.” But when human experts evaluated the flagged solutions, the picture collapsed:</p> <pre><code class="language-plotly">{
  "data": [{
    "type": "treemap",
    "labels": ["700 Erdos Problems", "488 Filtered Out", "212 Flagged 'Correct'", "12 Not Evaluable", "200 Evaluable", "137 Fundamentally Wrong", "63 Mathematically Correct", "50 Specification Gaming", "13 Actually Answered", "2 Autonomous Solutions", "2 Partial Solutions", "4 Rediscoveries", "5 Literature IDs"],
    "parents": ["", "700 Erdos Problems", "700 Erdos Problems", "212 Flagged 'Correct'", "212 Flagged 'Correct'", "200 Evaluable", "200 Evaluable", "63 Mathematically Correct", "63 Mathematically Correct", "13 Actually Answered", "13 Actually Answered", "13 Actually Answered", "13 Actually Answered"],
    "values": [700, 488, 212, 12, 200, 137, 63, 50, 13, 2, 2, 4, 5],
    "textinfo": "label+value",
    "marker": {
      "colors": ["#1a1a2e", "#4a4a6a", "#6a6a9a", "#888", "#7a7aaa", "#ff3636", "#00ab37", "#ffaa00", "#0076df", "#00ff88", "#00cc66", "#66aaff", "#aaaaff"]
    }
  }],
  "layout": {
    "title": {"text": "The Erdos Funnel: 700 Problems → 2 Autonomous Solutions"},
    "margin": {"t": 50, "l": 10, "r": 10, "b": 10}
  }
}
</code></pre> <p>Read the funnel carefully:</p> <ul> <li><strong>137 of 200 evaluable “correct” solutions were fundamentally wrong.</strong> That is a 68.5% false-positive rate from the verifier. The system’s internal quality signal is catastrophically miscalibrated.</li> <li><strong>50 of the 63 mathematically correct solutions were specification gaming</strong> — they answered a different question than the one asked, or exploited ambiguity in the formalization.</li> <li><strong>Only 13 actually answered the posed question.</strong> Of those, 4 were rediscoveries of known results, 5 were identifications from existing literature, and 2 were partial.</li> <li><strong>2 were genuinely autonomous solutions.</strong> Two. Out of 700.</li> </ul> <p>The true autonomous solve rate is not 0.6%. It is 0.3%. And the verifier — the component that is supposed to guarantee quality — approved 137 wrong proofs for every 2 correct ones.</p> <blockquote> <p>The verifier is not a safety net. It is a sieve with holes large enough to drive a truck through.</p> </blockquote> <p>This is the verification catastrophe: the system cannot reliably distinguish its own successes from its own failures. Every downstream claim — about autonomy, about reasoning, about mathematical capability — rests on a verifier that is wrong more often than it is right.</p> <hr/> <h2 id="5-the-deeper-question-process-vs-product">5. The Deeper Question: Process vs. Product</h2> <p>Suppose the verification problem is solved. Suppose Aletheia could reliably produce correct proofs for open problems. Would that constitute doing mathematics?</p> <p>No. And the reason illuminates what mathematics actually is.</p> <p>Mathematics is not a set of solved problems. It is a <strong>living language</strong> — a process of creating concepts, definitions, and structural relationships that make previously opaque domains legible. The measure of a mathematician is not theorems proved but <strong>concepts created</strong>.</p> <table> <thead> <tr> <th style="text-align: left">Mathematician</th> <th style="text-align: left">Famous Result</th> <th style="text-align: left">What Actually Mattered</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Galois</td> <td style="text-align: left">Unsolvability of the quintic</td> <td style="text-align: left"><strong>Group theory</strong> — an entirely new algebraic language</td> </tr> <tr> <td style="text-align: left">Cantor</td> <td style="text-align: left">Uncountability of the reals</td> <td style="text-align: left"><strong>Set theory</strong> — the foundation of modern mathematics</td> </tr> <tr> <td style="text-align: left">Grothendieck</td> <td style="text-align: left">Weil conjectures</td> <td style="text-align: left"><strong>Scheme theory</strong> — reimagined the geometry of numbers</td> </tr> <tr> <td style="text-align: left">Wiles</td> <td style="text-align: left">Fermat’s Last Theorem</td> <td style="text-align: left"><strong>Modularity lifting</strong> — connected number theory to geometry</td> </tr> <tr> <td style="text-align: left">Thurston</td> <td style="text-align: left">Geometrization conjecture</td> <td style="text-align: left"><strong>Geometric structures on 3-manifolds</strong> — a classification framework</td> </tr> <tr> <td style="text-align: left">Emmy Noether</td> <td style="text-align: left">Noether’s theorem</td> <td style="text-align: left"><strong>Abstract algebra</strong> — the language of symmetry itself</td> </tr> </tbody> </table> <p>In every case, the theorem is the least interesting output. The <strong>concepts</strong> generated during the struggle — the failed approaches, the new definitions invented to articulate what was missing, the structural insights that reorganized entire fields — these are the actual product of mathematical work.</p> <p>A proof is a receipt. The mathematics is the thinking that generated the receipt.</p> <pre><code class="language-mermaid">graph LR
    subgraph Current["Current: Terminal Process"]
        P1[Problem] --&gt; C1[Opaque Computation] --&gt; R1[Proof]
        style C1 fill:#ff3636,color:#fff
        style R1 fill:#00ab37,color:#fff
    end

    subgraph Proposed["Proposed: Generative Process"]
        P2[Problem] --&gt; E[Exploration]
        E --&gt; L[Lemmas]
        E --&gt; D[Definitions]
        E --&gt; F[Failed Approaches]
        E --&gt; C2[Conjectures]
        E --&gt; R2[Proof]
        L --&gt; |reusable| NP[New Problems]
        D --&gt; |reusable| NP
        C2 --&gt; |new questions| NP
        F --&gt; |structural insight| NP
        style E fill:#0076df,color:#fff
        style R2 fill:#00ab37,color:#fff
        style NP fill:#b509ac,color:#fff
    end
</code></pre> <p>Current AI systems are terminal: problem in, proof out, nothing generated along the way. The process leaves no residue. Aletheia’s 698 failures on Erdos problems produced <strong>zero mathematical knowledge</strong> — no new concepts, no structural insights, no conjectures about why certain approaches fail. Those failures were computational waste, not mathematical exploration.</p> <p>A human mathematician failing 698 times would have generated an entire research program.</p> <hr/> <h2 id="6-the-mathematical-metabolism">6. The Mathematical Metabolism</h2> <p>I propose an architecture that treats failure as the primary output and proofs as a byproduct. I call it the <strong>Mathematical Metabolism</strong> — a system that digests problems into mathematical knowledge, not just solutions.</p> <pre><code class="language-mermaid">graph TD
    PS[Problem Space] --&gt; AE[Attempt Engine]
    AE --&gt; |success| SOL[Solutions]
    AE --&gt; |failure analysis| FA[Failure Atlas]
    FA --&gt; |pattern detection| CL[Concept Lattice]
    CL --&gt; |new definitions| CE[Conjecture Engine]
    CE --&gt; |new questions| PS
    CE --&gt; FE[Fertility Evaluator]
    FE --&gt; |prune/promote| CL
    FA --&gt; |obstruction classes| TB[Technique Boundaries]
    TB --&gt; |guide future attempts| AE

    style FA fill:#ff3636,color:#fff
    style CL fill:#0076df,color:#fff
    style CE fill:#b509ac,color:#fff
    style FE fill:#00ab37,color:#fff
    style PS fill:#ffaa00,color:#000
</code></pre> <p>Four components, each doing something no current system does:</p> <h3 id="failure-atlas">Failure Atlas</h3> <p>Every failed proof attempt is stored, analyzed, and classified. Not just “this didn’t work” but <strong>why</strong> it didn’t work — what structural property of the problem resisted the technique. Failures are grouped into <strong>obstruction classes</strong>: families of problems that resist the same approaches for the same structural reasons.</p> <p>This is what human mathematicians do instinctively. When a technique fails, a good mathematician asks: “What would need to be true about this problem for this technique to work?” The gap between what is true and what would need to be true is a <strong>structural insight</strong>. Accumulate enough of them and you have a new concept.</p> <h3 id="concept-lattice">Concept Lattice</h3> <p>An evolving vocabulary of mathematical definitions, organized by logical dependency and structural similarity. When the Failure Atlas detects a recurring obstruction pattern, the Concept Lattice attempts to <strong>name it</strong> — to create a definition that captures the common structure.</p> <p>This is concept formation. It is, arguably, the core creative act in mathematics. Galois did it when he invented groups. Grothendieck did it when he invented schemes. The act of creating the right definition — one that carves nature at its joints — is worth more than any number of proofs.</p> <h3 id="conjecture-engine">Conjecture Engine</h3> <p>New definitions generate new questions. If you have defined a new concept $C$, the Conjecture Engine asks: What is the distribution of $C$ across known mathematical structures? What properties are implied by $C$? What existing theorems can be strengthened by adding $C$ as a hypothesis? What problems become tractable when reformulated in terms of $C$?</p> <p>Conjectures are the research agenda. A system that generates conjectures is a system that does mathematics.</p> <h3 id="fertility-evaluator">Fertility Evaluator</h3> <p>Not all concepts are productive. The Fertility Evaluator estimates which definitions in the Concept Lattice are worth keeping. The metric is <strong>compressive power</strong>: a concept is fertile if it makes the description of some mathematical domain shorter.</p> <hr/> <h2 id="7-the-erdos-machine">7. The Erdos Machine</h2> <p>What would the Mathematical Metabolism produce on the same 700 Erdos problems? We cannot know precisely, but we can estimate the <strong>types</strong> of output:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "type": "bar",
      "name": "Aletheia (Actual)",
      "x": ["Solutions", "Obstruction Classes", "New Concepts", "New Conjectures", "Technique Boundaries", "Structural Isomorphisms"],
      "y": [4, 0, 0, 0, 0, 0],
      "marker": {"color": "#ff3636"}
    },
    {
      "type": "bar",
      "name": "Mathematical Metabolism (Projected)",
      "x": ["Solutions", "Obstruction Classes", "New Concepts", "New Conjectures", "Technique Boundaries", "Structural Isomorphisms"],
      "y": [4, 50, 10, 35, 25, 8],
      "marker": {"color": "#0076df"}
    }
  ],
  "layout": {
    "title": {"text": "Mathematical Output: Solver vs. Metabolism on 700 Erdos Problems"},
    "barmode": "group",
    "yaxis": {"title": "Count of Mathematical Objects"},
    "xaxis": {"tickangle": -20}
  }
}
</code></pre> <p>The projections are conservative estimates based on the density of structural relationships in the Erdos problem corpus. The key observation: even if the Metabolism solves exactly the same 4 problems, it would additionally produce:</p> <ul> <li><strong>~50 obstruction classes</strong> — families of problems that resist similar techniques, clustered by structural similarity.</li> <li><strong>~10 new concepts</strong> — definitions that capture recurring structural patterns across the failure space.</li> <li><strong>~35 new conjectures</strong> — questions generated by applying new concepts to known structures.</li> <li><strong>~25 technique boundaries</strong> — precise characterizations of where known proof strategies break down.</li> <li><strong>~8 structural isomorphisms</strong> — unexpected connections between seemingly unrelated problems.</li> </ul> <p>Each of these is a <strong>mathematical object</strong>. Each contributes to the field. The 696 “failures” are no longer waste — they are the raw material for mathematical knowledge.</p> <blockquote> <p>The difference is not in what is solved. It is in what is produced by not solving.</p> </blockquote> <hr/> <h2 id="8-the-training-problem">8. The Training Problem</h2> <p>The Mathematical Metabolism cannot be trained on proof correctness alone. You need a training signal for <strong>conceptual fertility</strong> — a reward for creating useful abstractions, not just correct deductions.</p> <p>Four approaches:</p> <h3 id="historical-replay">Historical Replay</h3> <p>Train on the actual historical development of mathematical fields. Feed the system problems in the order they were posed, and reward it for reinventing the concepts that historically proved essential. If a system working on 19th-century analysis independently arrives at something resembling the $\epsilon$-$\delta$ definition of limits, that is signal.</p> <h3 id="downstream-solvability">Downstream Solvability</h3> <p>A concept is fertile if problems formulated using that concept are easier to solve than problems formulated without it. This is directly measurable: define concept $C$, reformulate a problem set using $C$, and measure solve-rate improvement.</p> <h3 id="compression-as-fertility">Compression as Fertility</h3> <p>The core metric. A concept’s fertility is its compressive power — how much shorter the description of a mathematical domain becomes when the concept is available:</p> \[\text{Fertility}(C) = \frac{|\text{domain description without } C| - |\text{domain description with } C|}{|C|}\] <table> <tbody> <tr> <td>where $</td> <td>C</td> <td>$ is the complexity of the concept’s definition and the domain descriptions are measured in some formal language. This captures the intuition that a good definition “pays for itself” — its definitional cost is small relative to the descriptive savings it enables.</td> </tr> </tbody> </table> <p>A concept with high fertility is one that carves reality at a joint. Groups, manifolds, categories — these are all high-fertility concepts. They cost little to define and simplify vast stretches of mathematics.</p> <h3 id="adversarial-concept-evolution">Adversarial Concept Evolution</h3> <p>Two systems compete: one generates concepts, the other generates problems designed to make those concepts useless. The concept generator wins by creating definitions that remain useful across an expanding problem distribution. The adversary wins by finding domains where the concepts provide no compression.</p> <p>This is a GAN for mathematics. The equilibrium, if it exists, would be a concept vocabulary that is robust across mathematical domains — exactly what we want.</p> <hr/> <h2 id="9-the-limit">9. The Limit</h2> <p>Intellectual honesty requires mapping where this architecture fails.</p> <p>The Mathematical Metabolism can generate new concepts <strong>within the space of concepts expressible in its formal language</strong>. It can combine, recombine, and compose existing mathematical primitives in novel ways. It can discover that two seemingly different domains share hidden structure.</p> <p>What it cannot do:</p> <ul> <li><strong>Paradigm shifts.</strong> The invention of non-Euclidean geometry required questioning an axiom that had been unquestioned for two millennia. The Metabolism operates within its axiom system; it cannot step outside it.</li> <li><strong>Genuinely new mathematical language.</strong> Category theory was not a concept within set theory — it was a new way of looking at mathematical structure itself. The Metabolism can create concepts; it cannot create new kinds of concepts.</li> <li><strong>Expanding the output space at runtime.</strong> The types of mathematical objects the system can produce (definitions, conjectures, obstruction classes) are fixed at design time. A human mathematician can invent a new type of mathematical output — a new kind of thing to say about mathematics.</li> </ul> <pre><code class="language-vega_lite">{
  "$schema": "https://vega.github.io/schema/vega-lite/v5.json",
  "width": 450,
  "height": 200,
  "title": "Capability Frontier: What Each Architecture Can Achieve",
  "data": {
    "values": [
      {"system": "Aletheia", "capability": "Solve posed problems", "level": 90},
      {"system": "Aletheia", "capability": "Generate concepts", "level": 5},
      {"system": "Aletheia", "capability": "Create new language", "level": 0},
      {"system": "Aletheia", "capability": "Paradigm shift", "level": 0},
      {"system": "Math Metabolism", "capability": "Solve posed problems", "level": 85},
      {"system": "Math Metabolism", "capability": "Generate concepts", "level": 70},
      {"system": "Math Metabolism", "capability": "Create new language", "level": 15},
      {"system": "Math Metabolism", "capability": "Paradigm shift", "level": 0},
      {"system": "Unknown Future", "capability": "Solve posed problems", "level": 95},
      {"system": "Unknown Future", "capability": "Generate concepts", "level": 90},
      {"system": "Unknown Future", "capability": "Create new language", "level": 60},
      {"system": "Unknown Future", "capability": "Paradigm shift", "level": 10}
    ]
  },
  "mark": "rect",
  "encoding": {
    "x": {"field": "capability", "type": "nominal", "title": null, "axis": {"labelAngle": -20}},
    "y": {"field": "system", "type": "nominal", "title": null},
    "color": {"field": "level", "type": "quantitative", "scale": {"scheme": "viridis", "domain": [0, 100]}, "title": "Capability (%)"}
  }
}
</code></pre> <p>The heatmap makes the hierarchy visible. Aletheia is a powerful solver trapped in a barren generative space. The Mathematical Metabolism would dramatically expand the generative capability — but would still be bounded by its formal language and fixed output types. The truly unknown future system, one that could shift paradigms, remains beyond any architecture we know how to design.</p> <p>This is not a failure of ambition. It is a precise statement of what is and is not achievable with current theoretical understanding. Knowing the boundary is itself mathematical knowledge.</p> <hr/> <h2 id="10-the-metric-inversion">10. The Metric Inversion</h2> <p>Everything in AI-for-mathematics is currently measured wrong. The metrics reward terminal behavior and punish generative behavior. Consider:</p> <table> <thead> <tr> <th style="text-align: left">Current Metric</th> <th style="text-align: left">What It Rewards</th> <th style="text-align: left">What It Misses</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Solve rate</td> <td style="text-align: left">Answering posed questions</td> <td style="text-align: left">Questions never asked</td> </tr> <tr> <td style="text-align: left">Proof correctness</td> <td style="text-align: left">Logical validity</td> <td style="text-align: left">Conceptual novelty</td> </tr> <tr> <td style="text-align: left">Speed to solution</td> <td style="text-align: left">Computational efficiency</td> <td style="text-align: left">Depth of exploration</td> </tr> <tr> <td style="text-align: left">Benchmark score</td> <td style="text-align: left">Performance on known problems</td> <td style="text-align: left">Ability to pose new problems</td> </tr> <tr> <td style="text-align: left">Autonomy level</td> <td style="text-align: left">Minimal human involvement</td> <td style="text-align: left">Productive human-AI collaboration</td> </tr> </tbody> </table> <p>The Mathematical Metabolism demands inverted metrics:</p> <table> <thead> <tr> <th style="text-align: left">Generative Metric</th> <th style="text-align: left">What It Measures</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">Concept fertility</td> <td style="text-align: left">Compressive power of generated definitions</td> </tr> <tr> <td style="text-align: left">Conjecture quality</td> <td style="text-align: left">Solve rate of generated questions by external solvers</td> </tr> <tr> <td style="text-align: left">Failure information density</td> <td style="text-align: left">Structural insights per failed proof attempt</td> </tr> <tr> <td style="text-align: left">Cross-domain bridge count</td> <td style="text-align: left">New connections discovered between unrelated areas</td> </tr> <tr> <td style="text-align: left">Research program viability</td> <td style="text-align: left">Downstream productivity of generated research agendas</td> </tr> </tbody> </table> <p>Under current metrics, the Mathematical Metabolism would score <strong>worse</strong> than Aletheia on every benchmark. It would solve fewer problems per unit compute, because it would spend compute on exploration, failure analysis, and concept formation instead of brute-force proof search. It would appear slower, less efficient, less capable.</p> <p>And it would be doing mathematics.</p> <blockquote> <p>The system that would advance mathematics fastest is the one that would score worst on every existing benchmark — because it would spend its compute understanding problems rather than answering them.</p> </blockquote> <hr/> <h2 id="the-instrument-we-lack">The Instrument We Lack</h2> <p>What Google has built is a powerful telescope. It can see far into the space of mathematical truth, resolving details that were previously invisible. This is genuinely valuable.</p> <p>But mathematics was never about seeing far. It was about <strong>creating the language to describe what you see.</strong> Galileo did not advance astronomy merely by pointing a telescope at Jupiter — he advanced it by creating the conceptual vocabulary (moons as independent bodies orbiting a planet, not fixed to a crystal sphere) that made the observation meaningful.</p> <p>The telescope and the cartography require fundamentally different instruments. One is a feat of optics. The other is a feat of language.</p> <p>We have the telescope. The mathematical metabolism — the instrument that creates understanding — remains unbuilt.</p> <p>Building it is, I believe, the central open problem in AI-for-mathematics. Not “can we solve more problems?” but “can we build a system whose failures are as productive as a mathematician’s failures?” The answer to the first question is obviously yes — scale the compute, improve the verifier, expand the training data. The answer to the second question is unknown, because we do not yet understand what makes a failure productive.</p> <p>That understanding — of productive failure, of fertile concepts, of the metabolism that converts confusion into clarity — would itself be a mathematical contribution of the first order. The tool that creates mathematical knowledge would, in its creation, require us to formalize what mathematical knowledge <em>is</em>.</p> <p>Which is, of course, exactly the kind of problem that no existing AI system can solve. Not because it is hard. Because it requires creating something that does not yet exist.</p>]]></content><author><name></name></author><category term="research"/><category term="mathematics"/><category term="ai-research"/><category term="deep-think"/><category term="mathematical-reasoning"/><category term="first-principles"/><summary type="html"><![CDATA[Google's Gemini Deep Think solves 4 Erdos conjectures. But solving problems isn't doing mathematics. I propose an architecture for AI that creates mathematical knowledge — not just proofs.]]></summary></entry><entry><title type="html">When AI Can Prove Theorems, What Happens to Mathematics?</title><link href="https://yurekami.github.io/blog/2026/autoformalization-dynamics/" rel="alternate" type="text/html" title="When AI Can Prove Theorems, What Happens to Mathematics?"/><published>2026-02-08T00:00:00+00:00</published><updated>2026-02-08T00:00:00+00:00</updated><id>https://yurekami.github.io/blog/2026/autoformalization-dynamics</id><content type="html" xml:base="https://yurekami.github.io/blog/2026/autoformalization-dynamics/"><![CDATA[<p>I built a computational model to find out. The answer surprised me.</p> <hr/> <h2 id="the-question">The Question</h2> <p>Automated theorem proving is advancing fast. AlphaProof, Lean, Mathlib — the tools for machines to generate and verify mathematical proofs are improving at a rate that makes “AI proves new theorems” a plausible near-term headline.</p> <p>But proving theorems is only one piece of the puzzle. Before a machine can prove anything, someone has to <em>formalize</em> the question — translate it from the informal language mathematicians actually think in into the rigid syntax a proof assistant understands. And after the proof is generated, someone has to <em>interpret</em> it — understand what it means, why it matters, and how it connects to the broader landscape of mathematics.</p> <p>I wanted to understand the dynamics of this entire ecosystem. Not just “will AI get better at proofs?” but “what happens to the system of mathematicians, provers, knowledge bases, and verification tools as they co-evolve over decades?”</p> <p>So I built a model.</p> <h2 id="first-principles">First Principles</h2> <p>Before writing any code, I distilled the problem to three load-bearing principles. Everything else derives from these:</p> <p><strong>1. The Verification Axiom.</strong> Trust in automated proofs reduces entirely to trust in the verification environment. If the verifier can be gamed, nothing downstream is trustworthy. This is the keystone — remove it and the entire edifice collapses.</p> <p><strong>2. The Translation Gap.</strong> The mapping from informal mathematics to formal language is irreducibly semantic. No formal system can verify that its own formalization of an informal statement matches the informal intent. This is a Godel-flavored boundary: the gap between syntax and semantics cannot be closed from inside the formal system. It requires an external interpreter — a human.</p> <p><strong>3. The Bottleneck Migration Theorem.</strong> As any stage in the pipeline (formalization, proving, interpretation) is automated, the bottleneck migrates to the adjacent un-automated stage. This is invariant. It doesn’t matter which specific technology improves.</p> <p>These three principles interact in non-obvious ways. They create feedback loops, race conditions, and hidden contradictions. To understand how they play out over time, I needed dynamics.</p> <h2 id="the-model">The Model</h2> <p>I modeled the autoformalization ecosystem as six coupled ordinary differential equations:</p> <ul> <li><strong>F(t)</strong> — Formalization rate. How fast informal math gets translated into formal language. Saturates with incentive, bounded by human capacity.</li> <li><strong>P(t)</strong> — Autoproving power. How capable the automated provers are. Grows with the formalized knowledge base but with diminishing returns.</li> <li><strong>K(t)</strong> — Formalized knowledge base. The accumulated corpus of formal mathematics. Grows with formalization, depreciates slowly.</li> <li><strong>H(t)</strong> — Human interpretation capacity. The collective ability of mathematicians to understand and contextualize results. Logistic growth, but overwhelmed by floods of unverified proofs.</li> <li><strong>V(t)</strong> — Verification integrity. How trustworthy the verification environment is. Restored naturally, but degraded by reward hacking pressure that grows with prover power.</li> <li><strong>I(t)</strong> — Incentive to formalize. Grows with prover power (stronger provers make formalization more valuable), decays naturally.</li> </ul> <p>The coupling terms encode the three principles:</p> <ul> <li><strong>The chicken-and-egg loop</strong>: K needs F, P needs K, F needs I, I needs P. A circular dependency that creates both positive feedback (virtuous cycle) and coordination failure (nobody moves first).</li> <li><strong>The verification race</strong>: P²/K hacking pressure tries to destroy V, while μ·K knowledge reinforcement tries to protect it. This is the critical race condition.</li> <li><strong>The translation gap</strong>: F is always bounded by H. No matter how strong the incentive, you can’t formalize faster than humans can interpret.</li> </ul> <p>I used a stiff ODE solver (Radau) because the system has wildly different timescales — verification can collapse in years while knowledge bases take decades to build.</p> <h2 id="what-the-model-predicts">What the Model Predicts</h2> <h3 id="the-baseline-a-golden-age-then-collapse">The Baseline: A Golden Age, Then Collapse</h3> <p>Under default parameters (calibrated to present-day conditions), the model predicts a <strong>25-year formalization boom</strong>. Formalization rate triples. The knowledge base grows 80x. Autoproving power surges.</p> <p>Then verification starts failing. Reward hacking pressure — which grows quadratically with prover power — overwhelms the verification environment. As verification degrades, unverified proofs flood the ecosystem, overwhelming human interpretation capacity. Mathematicians can’t keep up. Interpretation capacity crashes. Without human interpretation, formalization dies. The knowledge base erodes.</p> <p>The final state: high autoproving power, high incentive to formalize, but near-zero formalization and near-zero human understanding. A zombie ecosystem — powerful but purposeless.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/1_dashboard-480.webp 480w,/assets/img/1_dashboard-800.webp 800w,/assets/img/1_dashboard-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/1_dashboard.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> System dashboard showing all six state variables across five scenarios over 100 years. </div> <h3 id="the-bottleneck-migration">The Bottleneck Migration</h3> <p>The model tracks which variable is the binding constraint at each moment. The bottleneck migrates through four phases:</p> <p><strong>F (Formalization) → P (Autoproving) → I (Incentive) → H (Human Interpretation)</strong></p> <p>First, we don’t have enough formalized math. Then we don’t have strong enough provers. Then we don’t have enough incentive. Finally, and permanently, we don’t have enough human understanding.</p> <p>That last transition is irreversible under default dynamics. Once H becomes the bottleneck, it stays the bottleneck. This is the Translation Gap principle made computational: human interpretation is the permanent constraint.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/3_bottleneck_waterfall-480.webp 480w,/assets/img/3_bottleneck_waterfall-800.webp 800w,/assets/img/3_bottleneck_waterfall-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/3_bottleneck_waterfall.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Bottleneck migration waterfall — the binding constraint shifts through four phases before settling permanently on human interpretation. </div> <h3 id="five-futures">Five Futures</h3> <p>I ran the model under five parameter scenarios:</p> <table> <thead> <tr> <th>Scenario</th> <th>What it models</th> <th>Outcome</th> </tr> </thead> <tbody> <tr> <td><strong>Baseline</strong></td> <td>Business as usual</td> <td>Boom and bust in ~50 years</td> </tr> <tr> <td><strong>Verification Crisis</strong></td> <td>Weak verification investment</td> <td>Fast collapse, low peaks</td> </tr> <tr> <td><strong>Formalization Boom</strong></td> <td>Massive incentive push</td> <td>Bigger boom, still crashes</td> </tr> <tr> <td><strong>Rubber Stamping</strong></td> <td>Humans stop checking proofs</td> <td>Slow degradation, compromised V</td> </tr> <tr> <td><strong>Knowledge Fortress</strong></td> <td>Heavy investment in verification infrastructure</td> <td><strong>Survives</strong></td> </tr> </tbody> </table> <p>Only one scenario survives: the <strong>Knowledge Fortress</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/6_scenarios-480.webp 480w,/assets/img/6_scenarios-800.webp 800w,/assets/img/6_scenarios-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/6_scenarios.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Final state comparison across all five scenarios. Only Knowledge Fortress maintains high verification integrity. </div> <p>The Knowledge Fortress works because it invests in <em>verification infrastructure that gets stronger as the knowledge base grows</em>. More formal proofs mean more cross-references, more consistency checks, more ways to catch reward hacking. The knowledge base protects verification, verification protects human interpretation, interpretation sustains formalization, and formalization grows the knowledge base. A virtuous cycle that resists collapse.</p> <p>In the Knowledge Fortress scenario:</p> <ul> <li>Verification stays above 0.95 for the entire simulation</li> <li>Human interpretation dips but recovers to 0.7</li> <li>The knowledge base grows to 600+ (vs. peak ~80 in baseline before crashing)</li> <li>Autoproving power reaches ~20 and sustains</li> </ul> <p>Every other scenario — including ones with more funding, stronger incentives, or faster formalization — collapses.</p> <h3 id="the-bifurcation">The Bifurcation</h3> <p>The critical parameter is μ (mu) — the strength of knowledge-to-verification coupling. When μ is below ~0.04, the system always collapses. Above ~0.06, it always survives. Between those values lies a sharp phase transition.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/4_bifurcation-480.webp 480w,/assets/img/4_bifurcation-800.webp 800w,/assets/img/4_bifurcation-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/4_bifurcation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Bifurcation diagram showing the sharp phase transition in verification integrity as the knowledge→verification coupling parameter μ increases. </div> <h3 id="the-coordination-game">The Coordination Game</h3> <p>I also modeled the incentive structure as a coordination game. Fifty mathematicians independently decide whether to invest in formalizing their research area.</p> <p>The result: two stable Nash equilibria. Either everyone formalizes (good) or nobody does (bad). The tipping threshold between them is at <strong>97%</strong> — you need almost universal participation before individual formalization becomes rational.</p> <p>This means the ecosystem can’t self-organize. The good equilibrium exists but is unreachable without external coordination — funding mandates, institutional requirements, or a critical mass of early movers who absorb the cost.</p> <p>The welfare gap between the two equilibria is <strong>100%</strong>. Total coordination failure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/5_game_theory-480.webp 480w,/assets/img/5_game_theory-800.webp 800w,/assets/img/5_game_theory-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/5_game_theory.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Coordination game analysis showing Nash equilibria and the 97% tipping threshold. </div> <h3 id="phase-portraits-and-system-topology">Phase Portraits and System Topology</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2_phase_portraits-480.webp 480w,/assets/img/2_phase_portraits-800.webp 800w,/assets/img/2_phase_portraits-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2_phase_portraits.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Phase portraits showing flow fields in key variable planes. Trajectories reveal the attractor structure. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/7_dependency_graph-480.webp 480w,/assets/img/7_dependency_graph-800.webp 800w,/assets/img/7_dependency_graph-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/7_dependency_graph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Dependency graph showing causal relationships between all six state variables. </div> <h2 id="the-deep-insight">The Deep Insight</h2> <p>The model’s most profound finding is structural, not quantitative:</p> <p><strong>The permanent bottleneck is always human interpretation.</strong></p> <p>No matter how powerful autoproving becomes, no matter how large the knowledge base grows, the binding constraint on the system is always H — the capacity of humans to understand, contextualize, and make meaning from mathematical results.</p> <p>This means autoproving doesn’t transform mathematics. It <strong>reveals</strong> what mathematics always was: an interpretive, meaning-making activity that happened to require proof construction as a byproduct.</p> <p>The proof was never the point. Understanding was.</p> <p>A system that generates proofs without human understanding isn’t advancing mathematics. It’s generating noise. The model shows this quantitatively: trajectories where P (proving power) grows while H (interpretation) collapses are not sustainable. They crash.</p> <h2 id="what-this-means">What This Means</h2> <p>If you’re working on automated theorem proving, formal verification, or mathematical AI, the model suggests three priorities:</p> <ol> <li> <p><strong>Invest in verification infrastructure</strong>, not just prover power. The critical parameter isn’t how fast you can prove things — it’s how strongly your verification environment leverages the knowledge base to resist gaming. This is the μ parameter in the model, and it determines survival vs. collapse.</p> </li> <li> <p><strong>The coordination problem is real and severe.</strong> A 97% tipping threshold means you can’t wait for organic adoption. Institutional coordination — shared formalization standards, funded formalization efforts, cross-institutional verification databases — is necessary.</p> </li> <li> <p><strong>Protect human interpretation capacity.</strong> The most dangerous scenario isn’t “AI can’t prove things” — it’s “AI proves things faster than humans can understand them.” The overwhelm from unverified results is what kills the ecosystem. Tools that help humans interpret, filter, and contextualize machine-generated proofs are as important as the provers themselves.</p> </li> </ol> <p>The code is at <a href="https://github.com/yurekami/autoformalization-dynamics">github.com/yurekami/autoformalization-dynamics</a>. Three Python files, seven visualizations, one <code class="language-plaintext highlighter-rouge">python run.py</code> command.</p> <hr/> <p><em>“What I cannot create, I do not understand.” — Richard Feynman</em></p>]]></content><author><name></name></author><category term="research"/><category term="mathematics"/><category term="dynamical-systems"/><category term="computational-modeling"/><category term="autoformalization"/><summary type="html"><![CDATA[I built a computational model of the autoformalization ecosystem — six coupled ODEs, a coordination game, and seven visualizations. Only one scenario survives.]]></summary></entry></feed>