cv:
  name: Yurekami
  label: LLM Infrastructure Engineer
  email: daniel@aerlabs.tech
  url: https://aerlabs.tech/
  image: ""
  summary: LLM infrastructure engineer at Aer Labs, working on the full stack — from distributed storage and post-training to scalable serving and efficient fine-tuning.

  social_networks:
    - network: GitHub
      username: yurekami

  sections:
    Experience:
      - company: Open Source
        position: Independent Contributor
        location: Remote
        start_date: 2025
        end_date: ""
        summary: Contributing across the full LLM infrastructure stack, bridging high-level application needs with low-level system performance.
        highlights:
          - "Distributed Storage & Communication: deepseek-ai/3FS, DeepGEMM, DeepEP, DualPipe, uccl"
          - "LLM Serving: vllm-project/vllm, sgl-project/sglang, skypilot-org/skypilot"
          - "Post-Training & RLHF: verl-project/verl, OpenRLHF/OpenRLHF"
          - "Fine-Tuning: unslothai/unsloth, hiyouga/LlamaFactory, THUDM/slime"
          - "Distributed Computing: ray-project/ray, NVIDIA/NeMo"
          - "API & Structured Outputs: BerriAI/litellm, 567-labs/instructor"
          - "Deployment & Orchestration: ai-dynamo/dynamo, llm-d/llm-d"

    Open Source Contributions:
      - company: deepseek-ai
        position: "3FS, DeepGEMM, DeepEP, DualPipe"
        summary: "High-performance distributed storage and communication libraries"
        highlights:
          - "3FS: Fix connected_count metric underflow in TransportPool::get() (#375)"
          - "DeepGEMM: fix SM90ArchSpec instead of SM100ArchSpec (#270)"
          - "DeepEP: Fix typo NVHSMEM → NVSHMEM in installation docs (#544)"
          - "DualPipe: Use strings in __all__ instead of class objects (#25)"

      - company: verl-project/verl
        position: Reinforcement Learning Framework for LLMs
        summary: "11 merged contributions across documentation, bug fixes, and configuration improvements"
        highlights:
          - "fix: use configured response_length as default max_tokens (#4703)"
          - "fix: use dp_size instead of world_size in _balance_batch (#4697)"
          - "fix(lora): use TOKEN_CLS task type for Critic model (#4695)"
          - "fix: use model_dump() for Pydantic serialization (#4706)"
          - "fix: deprecate rollout.mode config option (#4690)"

      - company: vllm-project/vllm & sgl-project/sglang
        position: LLM Serving Engines
        highlights:
          - "vLLM: Remove dead block_quant_to_tensor_quant function (#31294)"
          - "vLLM: Add type hints to TurnMetrics class (#30552)"
          - "SGLang: fix Grafana dashboard metrics prefix (#15758)"
          - "SGLang: Add AIME25 dataset support for simple_eval (#14990)"

      - company: BerriAI/litellm & skypilot-org/skypilot
        position: API Interface & Cloud Orchestration
        highlights:
          - "litellm: handle empty error objects in response conversion (#18493)"
          - "litellm: add output_text property to ResponsesAPIResponse (#18491)"
          - "skypilot: Fix lstrip('ssh-') bug in SSH Node Pools (#8417)"
          - "skypilot: Fix dashboard rounding < 1 CPU to 0 (#8424)"

      - company: NVIDIA/NeMo & uccl-project/uccl & ray-project/ray
        position: Distributed Computing
        highlights:
          - "NeMo: Use PurePosixPath for cross-platform path handling (#15238)"
          - "NeMo: Fix progress_printer using wrong variable (#15237)"
          - "uccl: Gracefully handle missing InfiniBand directory (#611)"
          - "uccl: Add NVIDIA CUDA examples for PyTorch DDP (#607)"
          - "ray: Fix miscellaneous typos across codebase (#59718)"

      - company: Fine-Tuning & Other
        position: "unsloth, LlamaFactory, slime, instructor, langchain, dynamo, llm-d"
        highlights:
          - "LlamaFactory: Fix race condition in LoggerHandler during multi-GPU training (#10156)"
          - "unsloth: Fix Boolean value of Tensor ambiguity error (#3790)"
          - "slime: add --custom-model-provider-path argument (#1239)"
          - "instructor: stop masking runtime ImportErrors in from_provider (#1975)"
          - "OpenRLHF: pass ring_attn_group to reward model forward (#1182)"
          - "formal-conjectures: use positive naturals in infimum (#1903)"

    Skills:
      - name: LLM Infrastructure
        level: Expert
        icon: fa-solid fa-server
        keywords: "vLLM, SGLang, SkyPilot, Ray, NeMo, litellm"

      - name: Distributed Systems
        level: Expert
        icon: fa-solid fa-network-wired
        keywords: "3FS, DeepEP, DeepGEMM, uccl, DualPipe, NCCL"

      - name: ML Training & Fine-Tuning
        level: Expert
        icon: fa-solid fa-brain
        keywords: "verl, OpenRLHF, unsloth, LlamaFactory, slime"

      - name: Languages & Tools
        level: Expert
        icon: fa-solid fa-code
        keywords: "Python, Rust, C++, CUDA, PyTorch, Git"

    Interests:
      - name: LLM Systems
        icon: fa-solid fa-tag
        keywords: "Distributed Training, Model Serving, RLHF, Efficient Fine-Tuning, Expert Parallelism, Structured Outputs"
